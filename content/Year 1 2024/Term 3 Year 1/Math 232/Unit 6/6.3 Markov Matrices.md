# What is a markov chain
- #### We consider "dynamical systems":  
	- ###### A finite set of variables whose values change over time. We can represent this as a state vector

# What is a state vector
- #### Its a vector that has an $n$ states and corralates to one another
	- ###### IE probability vector
		- we have three guys $A,B,C$ 
			- $0.3$ probability being in with A
			- 0.6 probability with B
			- 0.1 Probability with C
				- ###### Note probability vector adds to 1
- #### Ex
	- ![[6.3 Markov Matrices-20240725015632868.png|446]]
	- if the initial probability state is (0.6,0.4) what is the state after one transition?
$$
\begin{align}
\vec{x}=\begin{bmatrix}
0.6\\0.4
\end{bmatrix}
\end{align}
$$
# Markov chain
- ### Definition
	- ###### A markov chain on n states is a system whose state vectors are proabability vectors and the state vector at time 0,1,2,... are denoted $\vec{x}_{0},\vec{x}_{1},\vec{x}_{2},\dots,$where $\vec{x}_{0}$ is the initial probability vector, and $\vec{x}_{1},\vec{x}_{2},...$ are determined by
		- $\vec{x}_{1}=P\vec{x}_{0},\vec{x}_{2}=P\vec{x}_{1},\vec{x}_{3}=P\vec{x}_{2}$ 
- #### EX
	- ![[6.3 Markov Matrices-20240725020204814.png|402]]

# Steady state matrix
